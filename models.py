# Reference:
# https://github.com/easezyc/WSDM2022-PTUPCDR

import torch
import torch.nn.functional as F


class LookupEmbedding(torch.nn.Module):
    def __init__(self, uid_all, iid_all, emb_dim):
        super().__init__()
        self.uid_embedding = torch.nn.Embedding(uid_all, emb_dim)
        self.iid_embedding = torch.nn.Embedding(iid_all + 1, emb_dim)
        # コンストラクタ (__init__)
        # uid_all: 全ユーザーIDの総数。この数に基づいて、ユーザーIDの埋め込みが作成されます。
        # iid_all: 全アイテムIDの総数。この数に基づいて、アイテムIDの埋め込みが作成されます。
        # emb_dim: 埋め込みの次元数。これは、各ユーザーやアイテムの埋め込みベクトルのサイズを指定します。
        # このコンストラクタ内で、uid_embeddingとiid_embeddingという2つの埋め込みレイヤーが初期化されます。
        # ユーザーID用の埋め込みはuid_allの数だけ、アイテムID用の埋め込みはiid_all + 1の数だけ作成されます
        # （+1は通常、特定のアイテムID（例えば未知のID）を扱うために使われることがあります）。

    def forward(self, x):
        # フォワードメソッド (forward)
        # 入力xは、ユーザーIDとアイテムIDのペアを含むバッチデータです（例えば、形状が[batch_size, 2]のテンソル）。

        uid_emb = self.uid_embedding(x[:, 0].unsqueeze(1))
        # x[:, 0]でバッチ内の全てのユーザーIDを取得し、unsqueeze(1)で次元を追加。
        # これにより、埋め込みレイヤーが適切に動作するように形状を整えます。
        # uid_emb shape  torch.Size([256, 1, 10])　256はバッチサイズ、１はユーザーID１個を１０次元のベクトルで表す

        iid_emb = self.iid_embedding(x[:, 1].unsqueeze(1))
        # 同様に、x[:, 1]でアイテムIDを取得し、unsqueeze(1)で次元を追加。
        # iid_emb shape torch.Size([256, 1, 10]) 256はバッチサイズ、１はアイテムID１個、１０次元のベクトル　を表す

        emb = torch.cat([uid_emb, iid_emb], dim=1)
        # emb shape torch.Size([256, 2, 10])
        # uid_embeddingとiid_embeddingを使用して、それぞれのIDに対応する埋め込みベクトルを取得します。
        # torch.catを使用して、ユーザーの埋め込みとアイテムの埋め込みを連結（単に列を１つ追加しただけ）し、
        # 各ペアの合成埋め込みを生成します。この合成埋め込みは、
        # 次のニューラルネットワーク層への入力として使用される可能性があります。

        # torch.Size([256, 2, 10]) の出力は、emb というテンソルの形状を表しており、
        # uid_emb と iid_emb を結合した結果です。ここでの各数値の意味は次のようになります：

        # 256: これはバッチサイズです。つまり、このテンソルには256個の独立したデータサンプルが含まれています。
        # 各サンプルは特定のユーザーとアイテムのペアを表していると考えられます。

        # 2: この次元は結合されたユーザーIDとアイテムIDの埋め込みから成るベクトルの数です。
        # 具体的には、uid_emb と iid_emb が結合される際に、
        # 1つ目の位置（ユーザーの埋め込み）と2つ目の位置（アイテムの埋め込み）として並んでいます。
        # つまり、この2はユーザーとアイテムの埋め込みが各データサンプルごとに
        # 2つの別々のベクトルとして存在することを示しています。

        # 10: この次元は、各埋め込みベクトルのサイズ（特徴の数）を示しています。
        # この場合、ユーザーの埋め込みもアイテムの埋め込みもそれぞれ10次元のベクトルです。
        # すなわち、各ユーザーと各アイテムは10の特徴量で表されており、
        # これらの特徴量はモデルによって解釈や学習の対象となります。

        return emb

class SimpleLSTM(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(SimpleLSTM, self).__init__()
        # LSTM層の定義
        self.lstm = torch.nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, batch_first=True)
        # 最終出力を1ノードにするための線形層
        self.linear = torch.nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        # LSTM層を通過
        # x.shape = (batch_size, sequence_length, input_dim)
        lstm_out, (hidden, cell) = self.lstm(x)
        # 最後のタイムステップの隠れ状態を取得
        # lstm_out.shape = (batch_size, sequence_length, hidden_dim)
        last_hidden = lstm_out[:, -1, :]
        # 線形層を通過させて最終出力を得る
        output = self.linear(last_hidden)
        return output
    
class DeepMetaNet(torch.nn.Module):
    # DeepMetaNetは、シーケンスデータから重要な情報を抽出し、
    # その情報に基づいて意思決定を行うための洗練されたアテンションメカニズムを提供します。
    # このプロセスは、データの関連部分を強調し、不要な情報を排除することで、
    # モデルの予測精度を向上させます。

#   "emb_dim": 10,
#   "meta_dim": 50,

# WSDM2022-PTUPCDR
# 'dptupcdr_mae': 0.8642578125, 'dptupcdr_rmse': 1.1804701089859009
# self.event_K = torch.nn.Sequential(torch.nn.Linear(emb_dim, emb_dim), torch.nn.ReLU(),
#                                        torch.nn.Linear(emb_dim, 1, False))
# なんで１次元にしているか、それは、torch.nn.Softmax(dim=1)で、softmaxを使うためだ！

# self.event_softmax = torch.nn.Softmax(dim=1)

# self.decoder = torch.nn.Sequential(torch.nn.Linear(emb_dim, meta_dim), torch.nn.ReLU(),
#                                        torch.nn.Linear(meta_dim, emb_dim * emb_dim))

# WSDM2023-DPTUPCDR
# 'dptupcdr_mae': 0.8676555752754211, 'dptupcdr_rmse': 1.173569679260254
# 'dptupcdr_mae': 0.8638774156570435, 'dptupcdr_rmse': 1.167047739028930
# 'dptupcdr_mae': 0.8675783276557922, 'dptupcdr_rmse': 1.1753735542297363

    def __init__(self, emb_dim, meta_dim):
        super().__init__()
        # self.event_K = torch.nn.Sequential(
        #     torch.nn.Linear(emb_dim, 7),
        #     torch.nn.ELU(alpha=0.8),
        #     torch.nn.Dropout(p=0.1),

        #     torch.nn.Linear(7, 4),
        #     torch.nn.ELU(alpha=0.8),
        #     torch.nn.Dropout(p=0.1),

        #     torch.nn.Linear(4, 2),
        #     torch.nn.ELU(alpha=0.8),
        #     torch.nn.Dropout(p=0.1),

        #     torch.nn.Linear(2, 1, False),
        # )#入力が１０ノード（１２８個）、出力が１ノードであればいい

        # モデルパラメータの設定
        input_dim = emb_dim
        hidden_dim = 20  # LSTMの隠れ層の次元数
        output_dim = 1
        output_dim2 = emb_dim * emb_dim

        self.event_K = SimpleLSTM(input_dim, hidden_dim, output_dim)

        self.event_softmax = torch.nn.Softmax(dim=1)

        self.decoder = SimpleLSTM(input_dim, hidden_dim, output_dim2)

        # self.decoder = torch.nn.Sequential(
        #     torch.nn.Linear(emb_dim, 25),
        #     torch.nn.ELU(alpha=0.8),
        #     torch.nn.Dropout(p=0.2),

        #     torch.nn.Linear(25, 50),
        #     torch.nn.ELU(alpha=0.8),
        #     torch.nn.Dropout(p=0.2),

        #     torch.nn.Linear(50, 75),
        #     torch.nn.ELU(alpha=0.8),
        #     torch.nn.Dropout(p=0.2),

        #     torch.nn.Linear(75, emb_dim * emb_dim),
        # ) #入力が１０ノード（１２８個）、出力が１００ノードであればいい

    def forward(self, emb_fea, seq_index):
        # DeepMetaNetクラスのforward関数は、埋め込み特徴量 (emb_fea) と
        # シーケンスインデックス (seq_index) を入力として、ニューラルネットワークを通じて
        # 出力を生成するための処理を行います。ここでの処理は、特にアテンションメカニズムを
        # 用いて重要な特徴を抽出し、その情報をデコードするためのものです。
        #
        # emb_fea:
        # emb_fea は、評価３以上のポジティブシーケンス、
        # つまり、seq_index のアイテムID を iid_embedding した10次元のベクトル。
        # 入力される埋め込み特徴量で、通常はユーザーIDやアイテムIDから得られた埋め込みなど、
        # ネットワークの前段階で得られた特徴表現です。これらの特徴はベクトル形式で提供されます。

        # seq_index:
        # 評価３以上の２０個のポジティブシーケンス。３以上がない場合は、０で埋められている。
        # シーケンスインデックスで、どの特徴量が有効か、
        # あるいは無効（例えばパディングされた部分など）かを示します。
        # 通常、有効なデータは1以上の値を持ち、
        # 無効なデータ（例えば、シーケンスの終わりにパディングされた部分）は 0 を持ちます。
        #
        # 以下に各ステップの詳細を説明します。

        mask = (seq_index == 0).float()
        # 0 は、１にしたマスクを作成

        event_K = self.event_K(emb_fea)
        # ２０個のベクトルをFFNにかけてエンコード（エンコードしてニューロンを段階的に削減して潜在要素）

        t = event_K - torch.unsqueeze(mask, 2) * 1e8
        # ２０個のベクトルから大きいマイナスの値を引く

        att = self.event_softmax(t)
        # softmaxで重み１から０のアテンションベクトルを作成

        his_fea = torch.sum(att * emb_fea, 1)
        # アテンションベクトルと最初の２０個のベクトルをかけ合わせて、
        # 重要な（似ている）アイテム同士はより大きな値に
        # そうでないアイテム同士は小さい値に
        # になったコンテキストベクトル(10次元のベクトルが128個)を生成

        output = self.decoder(his_fea)
        # コンテキストベクトルをFFNにかけてデコード（潜在要素をデコードしてニューロンを増やす。10次元のベクトル128個をデコード）

        return output.squeeze(1)

        # emb_fea=
        # tensor([[[ 0.3680, -0.0705,  0.7753,  ...,  0.1011,  1.0485,  0.7759],
        #          [ 0.2564, -0.7378,  0.4846,  ..., -0.2380,  0.6345,  1.2386],
        #          [ 0.4914,  0.1296,  0.2148,  ..., -0.2384,  0.1064,  0.6794],
        #          ...,
        #          [ 0.5981, -0.4399,  0.3231,  ..., -0.4700,  0.9674,  0.2568],
        #          [ 0.5981, -0.4399,  0.3231,  ..., -0.4700,  0.9674,  0.2568],
        #          [ 0.5981, -0.4399,  0.3231,  ..., -0.4700,  0.9674,  0.2568]],

        #         [[-0.3742, -1.3351, -0.3480,  ..., -0.7915,  0.3304,  1.1610],
        #          [-1.2325, -0.7946,  2.0935,  ...,  0.1945,  0.4237,  0.6245],
        #          [ 0.5841, -0.0963,  0.7374,  ..., -0.7294,  0.3519,  0.0642],
        #          ...,
        #          [ 1.5175, -0.4118,  0.5920,  ..., -0.8428,  0.3586,  0.5699],
        #          [ 0.0399, -0.2385,  1.3152,  ...,  0.3321,  0.6370,  0.6765],
        #          [ 0.2573, -0.8091, -0.4821,  ...,  1.0405, -0.9835,  0.0808]],

        #         [[ 0.6245, -0.2431,  0.6569,  ..., -1.7872, -0.8783,  0.0130],
        #          [ 1.8116, -1.2140,  0.0737,  ...,  1.2825,  0.2909, -1.1932],
        #          [-0.4969,  2.5194, -1.1600,  ...,  0.0107, -0.4809,  0.1015],
        #          ...,
        #          [ 0.5981, -0.4399,  0.3231,  ..., -0.4700,  0.9674,  0.2568],
        #          [ 0.5981, -0.4399,  0.3231,  ..., -0.4700,  0.9674,  0.2568],
        #          [ 0.5981, -0.4399,  0.3231,  ..., -0.4700,  0.9674,  0.2568]],

        #         ...,

        #         [[ 2.6925,  0.9172, -0.2437,  ..., -0.8779, -0.2362,  0.9944],
        #          [ 0.6913, -0.3934,  0.0618,  ..., -0.0456,  0.1714,  0.2055],
        #          [ 0.5144, -0.1002,  1.9356,  ...,  0.5737,  1.2392,  0.2177],
        #          ...,
        #          [ 0.1837, -0.4421,  0.6039,  ...,  0.1145, -0.7021,  1.7781],
        #          [-1.0771,  0.7315, -0.0130,  ..., -0.9319,  0.5282,  0.1363],
        #          [-0.9221, -0.2116,  0.0905,  ..., -0.2663,  0.5576,  0.4111]],

        #         [[ 0.3194, -0.3408,  0.6760,  ..., -0.2466,  0.3856,  0.8093],
        #          [ 0.6027, -0.2750,  0.2982,  ...,  0.6440,  0.4194,  0.5948],
        #          [-0.2738,  1.4134,  0.2184,  ..., -0.2029, -0.8596,  0.7671],
        #          ...,
        #          [-0.7525,  0.0507,  0.4406,  ..., -0.6023, -0.9720,  1.7895],
        #          [ 0.5981, -0.4399,  0.3231,  ..., -0.4700,  0.9674,  0.2568],
        #          [ 0.5981, -0.4399,  0.3231,  ..., -0.4700,  0.9674,  0.2568]],

        #         [[-0.0039,  0.0228,  1.1302,  ..., -0.6388,  0.8229,  0.7193],
        #          [-0.2411, -0.3157,  1.4945,  ...,  0.3344,  0.8321,  0.3729],
        #          [ 0.1841, -0.3681,  0.6152,  ...,  0.2803,  0.1023,  0.9014],
        #          ...,
        #          [ 0.5981, -0.4399,  0.3231,  ..., -0.4700,  0.9674,  0.2568],
        #          [ 0.5981, -0.4399,  0.3231,  ..., -0.4700,  0.9674,  0.2568],
        #          [ 0.5981, -0.4399,  0.3231,  ..., -0.4700,  0.9674,  0.2568]]],
        #        device='cuda:0', grad_fn=<EmbeddingBackward0>)

        # mask = (seq_index == 0).float()

        # seq_index=
        # tensor([[ 7273, 20128, 49228,  ...,     0,     0,     0],
        #         [11951, 35026,  8082,  ..., 16941,  9571, 24274],
        #         [32383,  9884, 25831,  ...,     0,     0,     0],
        #         ...,
        #         [ 1962, 26689, 38792,  ..., 36449, 40406, 45037],
        #         [ 4228, 19384, 11481,  ...,   592,     0,     0],
        #         [41730, 45931, 48565,  ...,     0,     0,     0]], device='cuda:0')

        # mask=
        # tensor([[0., 0., 0.,  ..., 1., 1., 1.],
        #         [0., 0., 0.,  ..., 0., 0., 0.],
        #         [0., 0., 0.,  ..., 1., 1., 1.],
        #         ...,
        #         [0., 0., 0.,  ..., 0., 0., 0.],
        #         [0., 0., 0.,  ..., 0., 1., 1.],
        #         [0., 0., 0.,  ..., 1., 1., 1.]], device='cuda:0')
        # mask.shape=
        # torch.Size([128, 20])

        # 処理のステップ
        # マスキング:
        # seq_indexに基づいてマスクを作成します。
        # seq_indexが 0 の場所ではマスクが 1 に、それ以外は 0 になります。
        # これは浮動小数点数型にキャストされ、後の計算で使用します。
        # ここでのマスクは、特定のシーケンス要素を無視するために使用されます
        # （例えば、パディングされた要素など）。

        # mask = (seq_index == 0).float()によって、
        # seq_indexが0の場所でTrue（1.0）となるマスクを作成します。
        # これは、無効なデータ点を識別し、後のアテンションスコア計算時に
        # これらの点を無視するために使用されます。

        # print(emb_fea.shape)  torch.Size([128, 20, 10])　10次元が20アイテム
        # event_K = self.event_K(emb_fea)
        # event_K shape torch.Size([128, 20, 10])　　10次元が20アイテム
        # event_K=
        # tensor([[[ 3.5966e-01,  8.6709e-01,  1.6761e+00,  ..., -9.3415e-02,
        #            1.1880e+00,  1.3397e+00],　★１０次元が、★
        #          [ 6.4462e-01,  1.7978e-03,  1.2341e+00,  ..., -3.9198e-01,
        #            4.8423e-01,  1.5103e+00],
        #          [ 6.4080e-01,  1.1619e+00,  1.5208e+00,  ..., -1.4185e+00,
        #           -1.9376e-01, -5.6303e-01],
        #          ...,
        #          [ 3.6307e-01,  3.2484e-01,  8.9797e-01,  ..., -3.8486e-01,
        #            9.2780e-01,  4.4877e-02],
        #          [ 2.5744e-01,  3.5785e-01,  1.3858e+00,  ..., -1.0727e+00,
        #            9.5673e-01,  1.2024e-01],
        #          [ 3.7604e-01,  5.2817e-01,  1.0674e+00,  ..., -6.6906e-01,
        #            6.9360e-01,  1.5574e-01]],　★２０アイテム★

        #         [[ 3.1780e-01, -2.0731e-01,  8.2305e-01,  ..., -1.5618e+00,
        #            8.1458e-01,  1.4282e+00],
        #          [-5.3683e-01,  4.1679e-01,  2.4858e+00,  ..., -1.3420e+00,
        #            5.7220e-01,  6.6667e-01],
        #          [ 7.0028e-01,  3.9652e-01,  8.4981e-01,  ..., -7.8638e-01,
        #           -2.4217e-01,  3.3401e-01],
        #          ...,
        #          [ 1.9697e+00,  3.2238e-01,  8.8558e-01,  ..., -8.9340e-01,
        #            1.2041e-01,  2.3720e-01],
        #          [ 5.0084e-01,  1.1489e+00,  1.5558e+00,  ...,  2.4160e-02,
        #            1.3935e-01,  4.1198e-01],
        #          [-5.8123e-01, -4.9822e-01,  1.4216e-01,  ...,  2.4310e-01,
        #           -8.3009e-01, -4.6131e-01]],

        #         [[ 1.1521e+00,  4.5063e-01,  1.1154e+00,  ..., -2.0951e+00,
        #           -1.4702e+00, -9.9847e-02],
        #          [ 1.4656e+00, -1.0229e+00,  9.6214e-01,  ...,  1.0047e+00,
        #            3.0607e-01, -9.9003e-01],
        #          [-1.8113e+00,  3.9108e+00,  2.5626e-01,  ..., -1.0764e+00,
        #           -5.8373e-02, -4.9152e-01],
        #          ...,
        #          [ 5.7941e-01,  1.8981e-01,  1.1673e+00,  ..., -4.3921e-01,
        #            9.1481e-01,  2.1200e-01],
        #          [ 6.2938e-04,  7.6657e-01,  1.5896e+00,  ..., -1.0897e+00,
        #            7.7422e-01, -1.9671e-01],
        #          [ 4.6904e-01,  4.2363e-01,  1.2264e+00,  ..., -7.6220e-01,
        #            7.8532e-01,  9.5132e-03]],

        #         ...,

        #         [[ 2.5266e+00,  1.1625e+00,  3.1495e-01,  ..., -1.0298e+00,
        #           -1.2231e+00,  6.2910e-01],
        #          [ 2.0544e-01,  1.3767e+00,  1.2532e+00,  ..., -5.1989e-01,
        #           -1.6872e-01, -2.3789e-01],
        #          [-8.4173e-01,  1.0248e+00,  3.0236e+00,  ...,  8.4916e-01,
        #            1.4085e+00,  5.1160e-01],
        #          ...,
        #          [ 9.3950e-01,  3.9284e-01,  2.2118e+00,  ..., -1.5684e+00,
        #           -8.9391e-01,  1.8281e+00],
        #          [-7.9903e-01,  1.5622e+00,  9.7887e-01,  ..., -1.4606e+00,
        #           -2.8462e-02, -6.7055e-01],
        #          [-1.5540e+00,  2.8153e-01,  1.1485e+00,  ..., -9.8872e-01,
        #            6.7865e-01, -6.6717e-01]],

        #         [[ 1.0423e+00,  3.5833e-01,  1.4953e+00,  ..., -7.4125e-01,
        #            1.2966e-01,  1.5407e+00],
        #          [ 1.9313e-01,  4.4556e-01,  1.3405e+00,  ...,  3.3875e-01,
        #           -1.1065e-01, -2.2365e-01],
        #          [-6.0132e-01,  2.5012e+00,  1.9170e+00,  ..., -1.5306e+00,
        #           -1.1443e+00, -3.5771e-01],
        #          ...,
        #          [ 1.8160e-02,  1.8533e+00,  1.2166e+00,  ..., -1.8186e+00,
        #           -1.1183e+00,  1.7691e+00],
        #          [ 3.7102e-01,  6.5747e-01,  1.3123e+00,  ..., -6.7814e-01,
        #            9.5582e-01,  3.1166e-02],
        #          [ 6.4877e-01,  1.8281e-01,  1.0315e+00,  ..., -7.6396e-01,
        #            7.3330e-01,  6.0674e-01]],

        #         [[ 1.3169e-01,  9.0702e-01,  1.9491e+00,  ..., -1.0372e+00,
        #            8.0416e-01,  1.3972e+00],
        #          [-7.0308e-01,  9.7547e-01,  2.0017e+00,  ..., -3.0484e-01,
        #            8.5043e-01, -1.9016e-01],
        #          [ 3.6089e-01,  7.3317e-01,  2.1705e+00,  ..., -1.5520e+00,
        #           -5.0068e-03,  2.8799e-01],
        #          ...,
        #          [-6.2811e-02,  7.1470e-01,  1.2852e+00,  ..., -4.5173e-01,
        #            9.3669e-01, -4.1508e-01],
        #          [ 2.5895e-01,  2.7200e-01,  1.0240e+00,  ..., -8.5495e-01,
        #            9.0692e-01,  2.3582e-01],
        #          [ 1.9298e-01,  4.1692e-01,  1.2625e+00,  ..., -1.0473e+00,
        #            6.1948e-01, -9.3806e-03]]], device='cuda:0', grad_fn=<AddBackward0>)　１２８個ある


        # t = event_K - torch.unsqueeze(mask, 2) * 1e8
        # t shape torch.Size([128, 20, 10])

        # アテンションスコアの計算:
        # マスクされた値から非常に大きな負の値（ここでは-1e8）を引きます。
        # これにより、アテンションスコアが計算される際に、
        # マスクされた位置は無視されるようになります。

        # イベントの変換:
        # emb_fea（埋め込み特徴量）をself.event_K関数に通して変換します。
        # この変換は、おそらく内部的な線形変換や他の種類の
        # ニューラルネットワークレイヤーを
        # 利用して特徴を適切な空間にマッピングします。

        # event_K = self.event_K(emb_fea)で、埋め込み特徴量を線形変換
        # （または他の変換）してアテンションスコアの前段階を計算します。
        # t = event_K - torch.unsqueeze(mask, 2) * 1e8で、
        # マスクされた部分に非常に大きな負の値を加えることで、
        # これらの部分がソフトマックス関数においてほぼゼロの重みを持つようにします。

        # torch.unsqueezeは、PyTorchでテンソルに新しい次元を挿入するために
        # 使用される関数です。
        # この操作は、テンソルの形状を変更する際に特に有用で、
        # 特定の演算を行うためにテンソルの次元数を調整する必要がある場合に用いられます。

        # torch.unsqueezeの動作
        # 具体的には、torch.unsqueeze(mask, 2)の呼び出しは、
        # maskテンソルの第3次元の位置にサイズ1の新しい次元を挿入します。
        # これにより、元のテンソルがどのような次元であっても、
        # 指定された位置に新しい次元が追加され、テンソルの形状が変更されます。

        # 入力: maskテンソルが例えば [batch_size, seq_length] の形状をしているとします。
        # 操作: torch.unsqueeze(mask, 2)を実行すると、
        # maskの形状が [batch_size, seq_length, 1] になります。
        # この操作の目的
        # DeepMetaNetのコンテキストで考えると、
        # torch.unsqueeze(mask, 2)は、
        # マスクを他のテンソルと適切にブロードキャスト（自動的に次元を拡張して演算可能にすること）
        # できる形状に変形するために使用されます。
        # 例えば、maskを埋め込みベクトルや他の多次元テンソルと要素ごとに掛け算する場合、
        # maskの形状をその他のテンソルの形状に合わせる必要があります。

        # この場合、unsqueeze操作によって、maskテンソルが他のテンソルとの演算で
        # 正しくブロードキャストされるように形状が調整され、
        # 各シーケンス位置のマスク値が適切に適用されることを保証します。
        # このようにして、無効なデータポイント（例えばパディングされた部分）を
        # 計算から効果的に除外することが可能になります。

        # t=
        # tensor([[[ 3.5966e-01,  8.6709e-01,  1.6761e+00,  ..., -9.3415e-02,
        #            1.1880e+00,  1.3397e+00],
        #          [ 6.4462e-01,  1.7978e-03,  1.2341e+00,  ..., -3.9198e-01,
        #            4.8423e-01,  1.5103e+00],
        #          [ 6.4080e-01,  1.1619e+00,  1.5208e+00,  ..., -1.4185e+00,
        #           -1.9376e-01, -5.6303e-01],
        #          ...,
        #          [-1.0000e+08, -1.0000e+08, -1.0000e+08,  ..., -1.0000e+08,
        #           -1.0000e+08, -1.0000e+08],
        #          [-1.0000e+08, -1.0000e+08, -1.0000e+08,  ..., -1.0000e+08,
        #           -1.0000e+08, -1.0000e+08],
        #          [-1.0000e+08, -1.0000e+08, -1.0000e+08,  ..., -1.0000e+08,
        #           -1.0000e+08, -1.0000e+08]],

        #         [[ 3.1780e-01, -2.0731e-01,  8.2305e-01,  ..., -1.5618e+00,
        #            8.1458e-01,  1.4282e+00],
        #          [-5.3683e-01,  4.1679e-01,  2.4858e+00,  ..., -1.3420e+00,
        #            5.7220e-01,  6.6667e-01],
        #          [ 7.0028e-01,  3.9652e-01,  8.4981e-01,  ..., -7.8638e-01,
        #           -2.4217e-01,  3.3401e-01],
        #          ...,
        #          [ 1.9697e+00,  3.2238e-01,  8.8558e-01,  ..., -8.9340e-01,
        #            1.2041e-01,  2.3720e-01],
        #          [ 5.0084e-01,  1.1489e+00,  1.5558e+00,  ...,  2.4160e-02,
        #            1.3935e-01,  4.1198e-01],
        #          [-5.8123e-01, -4.9822e-01,  1.4216e-01,  ...,  2.4310e-01,
        #           -8.3009e-01, -4.6131e-01]],

        #         [[ 1.1521e+00,  4.5063e-01,  1.1154e+00,  ..., -2.0951e+00,
        #           -1.4702e+00, -9.9847e-02],
        #          [ 1.4656e+00, -1.0229e+00,  9.6214e-01,  ...,  1.0047e+00,
        #            3.0607e-01, -9.9003e-01],
        #          [-1.8113e+00,  3.9108e+00,  2.5626e-01,  ..., -1.0764e+00,
        #           -5.8373e-02, -4.9152e-01],
        #          ...,
        #          [-1.0000e+08, -1.0000e+08, -1.0000e+08,  ..., -1.0000e+08,
        #           -1.0000e+08, -1.0000e+08],
        #          [-1.0000e+08, -1.0000e+08, -1.0000e+08,  ..., -1.0000e+08,
        #           -1.0000e+08, -1.0000e+08],
        #          [-1.0000e+08, -1.0000e+08, -1.0000e+08,  ..., -1.0000e+08,
        #           -1.0000e+08, -1.0000e+08]],

        #         ...,

        #         [[ 2.5266e+00,  1.1625e+00,  3.1495e-01,  ..., -1.0298e+00,
        #           -1.2231e+00,  6.2910e-01],
        #          [ 2.0544e-01,  1.3767e+00,  1.2532e+00,  ..., -5.1989e-01,
        #           -1.6872e-01, -2.3789e-01],
        #          [-8.4173e-01,  1.0248e+00,  3.0236e+00,  ...,  8.4916e-01,
        #            1.4085e+00,  5.1160e-01],
        #          ...,
        #          [ 9.3950e-01,  3.9284e-01,  2.2118e+00,  ..., -1.5684e+00,
        #           -8.9391e-01,  1.8281e+00],
        #          [-7.9903e-01,  1.5622e+00,  9.7887e-01,  ..., -1.4606e+00,
        #           -2.8462e-02, -6.7055e-01],
        #          [-1.5540e+00,  2.8153e-01,  1.1485e+00,  ..., -9.8872e-01,
        #            6.7865e-01, -6.6717e-01]],

        #         [[ 1.0423e+00,  3.5833e-01,  1.4953e+00,  ..., -7.4125e-01,
        #            1.2966e-01,  1.5407e+00],
        #          [ 1.9313e-01,  4.4556e-01,  1.3405e+00,  ...,  3.3875e-01,
        #           -1.1065e-01, -2.2365e-01],
        #          [-6.0132e-01,  2.5012e+00,  1.9170e+00,  ..., -1.5306e+00,
        #           -1.1443e+00, -3.5771e-01],
        #          ...,
        #          [ 1.8160e-02,  1.8533e+00,  1.2166e+00,  ..., -1.8186e+00,
        #           -1.1183e+00,  1.7691e+00],
        #          [-1.0000e+08, -1.0000e+08, -1.0000e+08,  ..., -1.0000e+08,
        #           -1.0000e+08, -1.0000e+08],
        #          [-1.0000e+08, -1.0000e+08, -1.0000e+08,  ..., -1.0000e+08,
        #           -1.0000e+08, -1.0000e+08]],

        #         [[ 1.3169e-01,  9.0702e-01,  1.9491e+00,  ..., -1.0372e+00,
        #            8.0416e-01,  1.3972e+00],
        #          [-7.0308e-01,  9.7547e-01,  2.0017e+00,  ..., -3.0484e-01,
        #            8.5043e-01, -1.9016e-01],
        #          [ 3.6089e-01,  7.3317e-01,  2.1705e+00,  ..., -1.5520e+00,
        #           -5.0068e-03,  2.8799e-01],
        #          ...,
        #          [-1.0000e+08, -1.0000e+08, -1.0000e+08,  ..., -1.0000e+08,
        #           -1.0000e+08, -1.0000e+08],
        #          [-1.0000e+08, -1.0000e+08, -1.0000e+08,  ..., -1.0000e+08,
        #           -1.0000e+08, -1.0000e+08],
        #          [-1.0000e+08, -1.0000e+08, -1.0000e+08,  ..., -1.0000e+08,
        #           -1.0000e+08, -1.0000e+08]]], device='cuda:0', grad_fn=<SubBackward0>)

        # att = self.event_softmax(t)
        # att shape torch.Size([128, 20, 10])

        # ソフトマックス正規化:
        # self.event_softmaxを適用して、
        # アテンションスコア（すなわち、各要素の重要度）を正規化します。   
        # 計算された t にソフトマックス関数を適用し、
        # 正規化されたアテンションスコアを得ます。
        # これにより、有効なデータポイントのみが強調され、
        # 無効なデータポイントは無視されます。

        # att=
        # tensor([[[0.1756, 0.3261, 0.3170,  ..., 0.3765, 0.4841, 0.3908],
        #          [0.2335, 0.1373, 0.2037,  ..., 0.2793, 0.2395, 0.4634],
        #          [0.2326, 0.4379, 0.2714,  ..., 0.1001, 0.1216, 0.0583],
        #          ...,
        #          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        #          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        #          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],

        #         [[0.0329, 0.0224, 0.0273,  ..., 0.0114, 0.0810, 0.1237],
        #          [0.0140, 0.0418, 0.1439,  ..., 0.0142, 0.0636, 0.0578],
        #          [0.0482, 0.0410, 0.0280,  ..., 0.0247, 0.0282, 0.0414],
        #          ...,
        #          [0.1715, 0.0380, 0.0290,  ..., 0.0222, 0.0405, 0.0376],
        #          [0.0395, 0.0869, 0.0568,  ..., 0.0556, 0.0412, 0.0448],
        #          [0.0134, 0.0167, 0.0138,  ..., 0.0692, 0.0156, 0.0187]],

        #         [[0.3113, 0.0175, 0.0925,  ..., 0.0143, 0.0128, 0.2085],
        #          [0.4259, 0.0040, 0.0794,  ..., 0.3168, 0.0757, 0.0856],
        #          [0.0161, 0.5563, 0.0392,  ..., 0.0395, 0.0526, 0.1409],
        #          ...,
        #          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        #          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        #          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],

        #         ...,

        #         [[0.3811, 0.0305, 0.0168,  ..., 0.0212, 0.0074, 0.0482],
        #          [0.0374, 0.0378, 0.0430,  ..., 0.0353, 0.0212, 0.0203],
        #          [0.0131, 0.0266, 0.2523,  ..., 0.1387, 0.1028, 0.0429],
        #          ...,
        #          [0.0779, 0.0141, 0.1120,  ..., 0.0124, 0.0103, 0.1599],
        #          [0.0137, 0.0455, 0.0326,  ..., 0.0138, 0.0244, 0.0131],
        #          [0.0064, 0.0126, 0.0387,  ..., 0.0221, 0.0496, 0.0132]],

        #         [[0.0924, 0.0233, 0.0789,  ..., 0.0339, 0.0515, 0.1370],
        #          [0.0395, 0.0254, 0.0676,  ..., 0.0999, 0.0405, 0.0235],
        #          [0.0179, 0.1982, 0.1203,  ..., 0.0154, 0.0144, 0.0205],
        #          ...,
        #          [0.0332, 0.1037, 0.0597,  ..., 0.0115, 0.0148, 0.1722],
        #          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        #          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],

        #         [[0.0552, 0.0732, 0.0944,  ..., 0.0389, 0.1028, 0.1911],
        #          [0.0239, 0.0784, 0.0995,  ..., 0.0809, 0.1077, 0.0391],
        #          [0.0694, 0.0615, 0.1178,  ..., 0.0233, 0.0458, 0.0630],
        #          ...,
        #          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        #          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        #          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],
        #        device='cuda:0', grad_fn=<SoftmaxBackward0>)

        # コンテキストベクトルの計算:
        # アテンションスコア (att) と元の埋め込み特徴量 (emb_fea) を
        # 要素ごとに乗算し、
        # その結果をシーケンスの次元（第1次元）に沿って加算します。
        # これにより、アテンションに基づく加重平均が得られ、
        # シーケンス内の重要な情報が抽出されたコンテキストベクトル
        # （his_fea）が生成されます。

        # コンテキストベクトルの計算:
        # アテンションスコアと元の埋め込み特徴量を要素ごとに掛け合わせ、
        # その後、次元1に沿って合計を取ることで、入力データの加重平均を計算します。
        # この加重平均は、シーケンス内の重要な情報を含んだコンテキストベクトル
        # となります。

        # his_fea = torch.sum(att * emb_fea, 1)
        # print(his_fea.shape)  torch.Size([128, 10])

        # his_fea=
        # tensor([[ 0.4904, -0.1036,  0.4391,  ..., -0.0900,  0.7440,  0.9722],
        #         [ 0.9181, -0.0334,  0.9352,  ...,  0.3694,  0.6811,  0.7063],
        #         [ 1.0396,  2.0093,  1.4461,  ...,  0.7949,  1.1819, -0.3099],
        #         ...,
        #         [ 1.1625,  1.4959,  0.8709,  ...,  0.5565,  1.1571,  1.3697],
        #         [ 0.5594,  0.5551,  0.5537,  ...,  0.2218,  0.7480,  1.0259],
        #         [ 0.6634,  0.0169,  0.9524,  ...,  0.3546,  0.7591,  0.5983]],
        #     device='cuda:0', grad_fn=<SumBackward1>)

        # デコーディング:
        # コンテキストベクトルをself.decoderに通して、最終的な出力を生成します。
        # デコーダーは、たとえば線形層や活性化関数などから成る可能性があります。
        # これは、例えばクラス分類や次のアイテムの推薦など、
        # 具体的なタスクに応じた予測値です。

        # output = self.decoder(his_fea)
        # print(output.shape)  torch.Size([128, 100])　　
        #   torch.nn.Linear(75, emb_dim * emb_dim),　emb_dim = 10 x 10 だから100。

        # output=
        # tensor([[ 0.1034, -0.0815, -0.2614,  ...,  0.0784, -0.0223, -0.1004],
        #         [-0.0311,  0.0344, -0.1689,  ...,  0.0609,  0.0153, -0.0809],
        #         [ 0.0266, -0.1058, -0.1202,  ...,  0.0796,  0.0702, -0.2009],
        #         ...,
        #         [ 0.0510, -0.1013, -0.1072,  ..., -0.0452,  0.0552, -0.2064],
        #         [-0.0092,  0.0059, -0.0775,  ...,  0.0552, -0.0721,  0.0105],
        #         [ 0.0503, -0.0767, -0.2102,  ...,  0.0805, -0.0364,  0.0051]],
        #     device='cuda:0', grad_fn=<AddmmBackward0>)
        
        return output.squeeze(1)
        # 出力の整形:
        # 最終的な出力をsqueezeメソッドで次元削減し、余分な次元を取り除きます。

        # Tensor.squeeze()は「テンソル中の要素数1の次元を削除する」、 
        # Tensor.unsqueeze(d: int)は「テンソルのd階目に要素数1の次元を挿入する」 というものだ。
        # 要素数1の次元の扱いはテンソル操作上面倒になるポイントなので、 
        # 使い方を覚えると実はこれらはうれしい関数なのだ。
        # https://wonderhorn.net/programming/squeeze.html


        # このforward関数は、アテンションメカニズムを活用して
        # 入力シーケンスから重要な特徴を選択し、
        # それを基に最終的な予測や分類を行うための出力を生成しています。
        # シーケンスデータを扱う際に、このようなアプローチは特に有効で、
        # 文脈に依存する重要な情報を抽出するのに役立ちます。


class Model(torch.nn.Module):
    def __init__(self, uid_all, iid_all, num_fields, emb_dim, meta_dim_0):
        super().__init__()
        self.num_fields = num_fields
        self.emb_dim = emb_dim
        # src_model は、オブジェクト。LookupEmbeddingのインスタンスを初期化
        self.src_model = LookupEmbedding(uid_all, iid_all, emb_dim)
        self.tgt_model = LookupEmbedding(uid_all, iid_all, emb_dim)
        self.aug_model = LookupEmbedding(uid_all, iid_all, emb_dim)
        # self.deep_meta_net = DeepMetaNet(emb_dim, meta_dim_0)
        self.deep_meta_net = DeepMetaNet(
            emb_dim, meta_dim_0, hidden_dim=emb_dim, num_layers=2
        )
        self.mapping = torch.nn.Linear(emb_dim, emb_dim, False) #

    def forward(self, x, stage):
        if stage == "train_src":  #=====CDR Pretraining=====
            emb = self.src_model.forward(x) 
            # ここは、self.src_model(x) と書けるらしい
            x = torch.sum(emb[:, 0, :] * emb[:, 1, :], dim=1) 
            # ユーザー埋め込みとアイテム埋め込みを掛けて y を求める
            return x
        

        elif stage in ["train_tgt", "test_tgt"]:#=========TgtOnly========
            emb = self.tgt_model.forward(x)
            x = torch.sum(emb[:, 0, :] * emb[:, 1, :], dim=1) 
            # ユーザー埋め込みとアイテム埋め込みを掛けて y を求める
            return x
        
        
        elif stage in ["train_aug", "test_aug"]:#=========DataAug========
            emb = self.aug_model.forward(x)
            x = torch.sum(emb[:, 0, :] * emb[:, 1, :], dim=1) 
            # ユーザー埋め込みとアイテム埋め込みを掛けて y を求める
            return x

        elif stage in ["train_meta", "test_meta"]:#==========Deep PTUPCDR==========

    #   --------------------------------------------------------------------------------
    #   元のソースコードは、これだけ。この部分だけのコードの意味を記載した。この部分より後のコメントは、
    #   もっと詳しい解説。実際のデータなどを出力しているので、非常に長いので、読むのが大変だけど、
    #   より詳しい情報が得られる。
    #   uid_emb_src = self.src_model.uid_embedding(x[:, 0].unsqueeze(1)) 
    #   # x[:, 0] は、tgt user だが、meta は src と共通だから、src_modelでいいのか？ src の埋め込みを作成

    #   iid_emb_ufea_src = self.src_model.iid_embedding(x[:, 2:]) 
    #   # x[:, 2:] は、src の pos seq(iid のリスト)だから、posのiidの埋め込みを作成

    #   mapping = self.deep_meta_net.forward(iid_emb_ufea_src, x[:, 2:]).view(-1, self.emb_dim, self.emb_dim)
    #   # pos の iid の埋め込みと、srcのpos seq(iid のリスト)から mapping (つまり、bridge)関数を作成。
      
    #   uid_emb_src_tgt = torch.bmm(uid_emb_src, mapping) 
    #   # src のuser 埋め込み に mapping(つまり、bridge)関数をかけることで、src のuser 埋め込みに嗜好を転移して 
    #   # ターゲットドメインでの 初期のuser埋め込みとなる uid_emb_src_tgt を作成

    #   iid_emb_tgt = self.tgt_model.iid_embedding(x[:, 1].unsqueeze(1))
    #   # x[:, 1] は、tgt の iid から tgt の埋め込み iid_emb_tgt を作成
        
    #   emb = torch.cat([uid_emb_src_tgt, iid_emb_tgt], 1)
    #   # uid_emb_src_tgt to iid_emb_tgt を結合して行列を作成
      
    #   output = torch.sum(emb[:, 0, :] * emb[:, 1, :], dim=1)
    #   # 行列の1列名(user)と2列目(item)の内積を取って、yを求める
      
    #   return output
    #   --------------------------------------------------------------------------------

            # 目的と利点
            # このステージでの処理は、メタデータや追加的な情報を利用してユーザーと
            # アイテムの埋め込みをより表現豊かに変換することを目的としています。
            # このような処理により、単純なユーザーIDやアイテムIDの埋め込みだけでは捉えきれない、
            # 複雑な相互作用や特性をモデルが学習できるようになります。
            # 特にレコメンデーションシステムや個別化されたサービスを提供するアプリケーションにおいて、
            # このような豊かな特徴表現は、より精度の高い予測や効果的なユーザー体験を実現するために重要です。


            # 入力テンソル x
            # x=
            # torch.Size([128, 22])

            # x=         uid     iid     ３以上のiidの２０個の列   が  128 行
            # tensor([[ 45393,  62614,   7273,  ...,      0,      0,      0],
            #         [ 76570,  92165,  11951,  ...,  16941,   9571,  24274],
            #         [110589, 106795,  32383,  ...,      0,      0,      0],
            #         ...,
            #         [ 19201,  69613,   1962,  ...,  36449,  40406,  45037],
            #         [ 27574,  91570,   4228,  ...,    592,      0,      0],
            #         [177134, 113298,  41730,  ...,      0,      0,      0]],
            #        device='cuda:0')


            # elif stage in ['train_meta', 'test_meta']: の部分では、
            # Modelクラスの forward 関数がメタデータの
            # トレーニングまたは
            # テストステージでの処理を行っています。
            # このステージでは、
            # ユーザーとアイテムの埋め込みからメタ情報を利用して新しい特徴表現を
            # 生成するプロセスが含まれています。
            # 
            # 具体的には以下の手順で処理が行われます。

            uid_emb_src = self.src_model.uid_embedding(x[:, 0].unsqueeze(1)) # (1) Us
            # ソースモデルのユーザー埋め込み抽出:
            # self.src_model.uid_embedding(x[:, 0].unsqueeze(1)) で、
            # 同じく入力テンソル x の最初の列（ユーザーID）からユーザーの埋め込み（ベクトル）だけを抽出。
            # print("uid_emb_src")
            # print(uid_emb_src.shape)   #torch.Size([128, 1, 10])　128 は、バッチサイズ、1 が 列で、10が次元
            # 10個の実数値で表されたベクトル１個でユーザーを表して１２８行ならぶ。

            iid_emb = self.tgt_model.iid_embedding(x[:, 1].unsqueeze(1)) # (1) Vt
            # ターゲットモデルのアイテムの埋め込みを抽出:
            # self.tgt_model.iid_embedding(x[:, 1].unsqueeze(1)) は、
            # 入力テンソル x の2番目の列（アイテムID）からアイテムの埋め込み（ベクトル）だけを抽出。
            # ここで unsqueeze(1) は次元の形状を調整しています。
            # print("stage train_meta")
            # print("iid_emb")
            # print(iid_emb.shape) #torch.Size([128, 1, 10])　　128 は、バッチサイズ、1 が アイテムで、10が次元
            # 10個の実数値で表されたベクトル１個でアイテムを表して１２８行ならぶ。

            ufea = self.src_model.iid_embedding(x[:, 2:]) # x[:, 2:] (１)　interacted itmes in source domain
            # 
            # self.src_model.iid_embedding が Characteristic Encoder
            # ufea が Pu
            # ufea は、ポジティブシーケンスのアイテムIDをiid_embeddingで、10次元のベクトルに変換している。
            # ソースモデルからiid_embedding関数を使って
            # 追加的なアイテムの埋め込みを抽出（3列目以降のデータ（pos_seq）positive sequence）:
            # self.src_model.iid_embedding(x[:, 2:]) は、
            # 入力テンソル x の3列目以降のデータ（pos_seq）から追加的な埋め込みを抽出しています。
            # print("ufea")
            # print(ufea.shape) #torch.Size([128, 20, 10])　　これが２０列あるのがよくわからない。
            # y + 20 なら、21のはず　→　おそらく y はなく、評価３以上のアイテムが２０個だと思う。
            # つまり、10個の実数値で表されたベクトル２０個でアイテムを表して１２８行ならぶ。



            mapping = self.deep_meta_net.forward(ufea, x[:, 2:]).view(-1, self.emb_dim, self.emb_dim)

            # mapping= self.deep_meta_net.forward が (3) Meta nerwork

            # Meta nerworkに対して、ufea と x[:, 2:] のデータを渡している。
            # .view は何をしているか？

            # view(-1, self.emb_dim, self.emb_dim)は、
            # PyTorchのテンソル操作であり、
            # テンソルの形状を再構成するために使用されます。
            # この具体的な使用例では、
            # DeepMetaNetからの出力を特定の形状に変形する目的で使われています。

            # view関数の説明
            # view関数は、テンソルの要素数を保持しながら、
            # その形状（次元数と各次元のサイズ）を変更します。
            # 引数には新しい形状を指定します。
            # 
            # view(-1, self.emb_dim, self.emb_dim)
            #
            # -1: この引数は、その次元のサイズを自動的に計算させるために使われます。
            # 　　　PyTorchは他の指定された次元のサイズからこの次元のサイズを推測します。

            # self.emb_dim: この値は、各次元のサイズを表し、ここでは埋め込みの次元数が用いられます。

            # 使用例のコンテキスト
            # この行において、DeepMetaNetのforward関数からの出力がどのような形状であっても、
            # 最終的に[-1, self.emb_dim, self.emb_dim]という形状に変形されることを意味します。

            # この変形は、以下の目的で行われることが一般的です：

            # 適切な形状への整形: モデルの後続の層や操作で必要とされる特定の形状にデータを整形します。
            # 　　　　　　　　　　特に、マトリックス演算を行う際や、特定の形状が要求される層（例えば、畳み込み層など）へ
            # 　　　　　　　　　　の入力として用いる場合に重要です。

            # データのバッチ処理: -1を使用することで、どのようなバッチサイズのデータであっても、
            # 　　　　　　　　　　指定されたemb_dimに基づいて適切に処理することができます。これにより、
            # 　　　　　　　　　　モデルが異なるサイズの入力データに対して柔軟に対応できるようになります。

            # 次元の明示: この操作によって、データが具体的にどのような情報を持っているか（この場合は、
            # 　　　　　　emb_dim x emb_dimのマトリックスとしての情報）が明確になり、後続の計算や解析が容易になります。

            # このように、view(-1, self.emb_dim, self.emb_dim)はテンソルの形状を効率的に再構成し、
            # モデルの処理フローにおいてデータが適切に扱われるようにするための重要なステップです。


        # output=
        # tensor([[ 0.1034, -0.0815, -0.2614,  ...,  0.0784, -0.0223, -0.1004],
        #         [-0.0311,  0.0344, -0.1689,  ...,  0.0609,  0.0153, -0.0809],
        #         [ 0.0266, -0.1058, -0.1202,  ...,  0.0796,  0.0702, -0.2009],
        #         ...,
        #         [ 0.0510, -0.1013, -0.1072,  ..., -0.0452,  0.0552, -0.2064],
        #         [-0.0092,  0.0059, -0.0775,  ...,  0.0552, -0.0721,  0.0105],
        #         [ 0.0503, -0.0767, -0.2102,  ...,  0.0805, -0.0364,  0.0051]],

            # tensor([[[ 0.1034, -0.0815, -0.2614,  ..., -0.0239, -0.2055, -0.1000],
            #          [ 0.1454, -0.0760, -0.2429,  ..., -0.0755, -0.0648,  0.0382],
            #          [-0.1232, -0.0646,  0.0288,  ..., -0.1268, -0.0781,  0.0097],
            #          ...,
            #          [ 0.0102, -0.0689, -0.1407,  ...,  0.0182, -0.1767, -0.0105],
            #          [-0.1422,  0.0451,  0.0053,  ...,  0.1063, -0.0622, -0.0500],
            #          [ 0.1558,  0.0286,  0.2551,  ...,  0.0784, -0.0223, -0.1004]],

            #         [[-0.0311,  0.0344, -0.1689,  ...,  0.0875, -0.1767, -0.1710],
            #          [ 0.1296, -0.1267, -0.1744,  ..., -0.0011,  0.0331,  0.0645],
            #          [-0.1747, -0.0329,  0.1011,  ..., -0.1096, -0.0363,  0.0198],
            #          ...,
            #          [ 0.0795, -0.1545, -0.0611,  ...,  0.0720, -0.0621, -0.1206],
            #          [-0.0191, -0.1434,  0.0391,  ...,  0.1734, -0.0474, -0.0571],
            #          [ 0.2056,  0.1224,  0.2261,  ...,  0.0609,  0.0153, -0.0809]],

            #         [[ 0.0266, -0.1058, -0.1202,  ...,  0.0072, -0.1765, -0.0161],
            #          [ 0.1123,  0.0533, -0.1839,  ..., -0.0816,  0.0022,  0.1494],
            #          [-0.1319, -0.0169, -0.0426,  ..., -0.1229, -0.1613,  0.0137],
            #          ...,
            #          [ 0.0061, -0.1171, -0.0782,  ..., -0.0880, -0.2118, -0.1035],
            #          [-0.0102, -0.0433, -0.0291,  ...,  0.0041, -0.1302, -0.1534],
            #          [ 0.2150,  0.0195,  0.2107,  ...,  0.0796,  0.0702, -0.2009]],

            #         ...,

            #         [[ 0.0510, -0.1013, -0.1072,  ...,  0.0038, -0.0916, -0.0858],
            #          [ 0.0800, -0.0268, -0.0362,  ..., -0.0158, -0.1042,  0.1251],
            #          [-0.2154,  0.0775, -0.0755,  ..., -0.0754,  0.0162,  0.0649],
            #          ...,
            #          [ 0.0917, -0.1279, -0.0457,  ..., -0.0385, -0.1543,  0.0053],
            #          [ 0.0026, -0.1423, -0.0598,  ...,  0.0745, -0.1385, -0.1545],
            #          [ 0.0285,  0.0004,  0.1946,  ..., -0.0452,  0.0552, -0.2064]],

            #         [[-0.0092,  0.0059, -0.0775,  ...,  0.1422, -0.1477, -0.0974],
            #          [ 0.0993, -0.0004, -0.0872,  ..., -0.1609, -0.0767, -0.0470],
            #          [-0.1943,  0.0696, -0.0006,  ..., -0.0352, -0.0053,  0.0684],
            #          ...,
            #          [-0.0371, -0.0861, -0.0598,  ...,  0.0658, -0.1553,  0.0425],
            #          [-0.0756, -0.0703,  0.0591,  ...,  0.1605, -0.0973, -0.0559],
            #          [ 0.0352,  0.0604,  0.3173,  ...,  0.0552, -0.0721,  0.0105]],

            #         [[ 0.0503, -0.0767, -0.2102,  ...,  0.0245, -0.3350, -0.0722],
            #          [ 0.1866, -0.0876, -0.1930,  ..., -0.0106, -0.0126, -0.0545],
            #          [-0.1685, -0.1818,  0.0172,  ..., -0.1045, -0.2115,  0.0950],
            #          ...,
            #          [-0.0696, -0.0906, -0.0192,  ..., -0.0047, -0.2903, -0.2523],
            #          [-0.0635, -0.0273,  0.0345,  ...,  0.0345, -0.0556, -0.0869],
            #          [ 0.1992,  0.1236,  0.2873,  ...,  0.0805, -0.0364,  0.0051]]],
            #        device='cuda:0', grad_fn=<ViewBackward0>)
            # 

            # print("mapping")
            # print(mapping.shape) #torch.Size([128, 10, 10])

            # DeepMetaNetモデルを通じての特徴変換:
            # self.deep_meta_net.forward(ufea, x[:, 2:]) は、
            # 上で抽出した追加的な埋め込み ufea をDeepMetaNetモデルに渡し、変換を行っています。
            # この変換は、メタデータを使用してより豊かな特徴表現を生成するために使用されます。

            # ここで、x[:, 2:]はDeepMetaNetに渡されるシーケンスインデックスの部分であり、
            # このインデックスがどの特徴量を取り扱うかを示しています。
            # 具体的には、このインデックスを使って、DeepMetaNet内でアテンションのマスキングや、
            # 特定の特徴量を抽出する際の計算に利用されます。

            # 処理の説明
            # train_metaおよびtest_metaステージでは、
            # 入力xから特定のカラムを抽出してそれぞれの埋め込みを取得し、
            # DeepMetaNetを用いて重要な特徴を抽出する処理が行われます。
            # ufeaは、おそらくユーザーの特徴やその他のメタデータの埋め込みを表し、
            # これに基づいてアテンションが計算されます。
            # 最終的に、得られた特徴のマッピングと元のユーザー埋め込みを組み合わせて、
            # 予測や分類のための最終的な出力が生成されます。
            # このように、seq_indexはModelクラスの特定ステージにおいて重要な役割を果たしており、
            # モデルがどの情報に注目すべきかを制御するために使用されています。

            # Modelクラスのforward関数において、DeepMetaNetのforwardメソッドに渡される
            # seq_indexはx[:, 2:]となっています。
            # これにより、入力テンソルxの最初の2列を除いた残りの部分がシーケンスインデックスとして扱われます。

            # コンテキストと処理
            # x: 入力テンソルxは、複数の特徴量を含む可能性があります。
            # 最初の2列は通常、ユーザーIDとアイテムIDに相当すると考えられます。
            # 残りの列（x[:, 2:]）は、追加の特徴量やメタデータを含んでいる可能性があり、
            # これがシーケンスインデックスとしてDeepMetaNetに渡されます。
            # DeepMetaNetでのseq_indexの役割
            # DeepMetaNetのforward関数においてseq_index（ここではx[:, 2:]）は、
            # 特定の処理やマスキングに使用される可能性があります。
            # 例えば、無効なデータ（パディングされたデータなど）を識別し、
            # 適切なアテンション重みを計算する際に無視するために使われることがあります。

            # 処理の流れ
            # DeepMetaNetの処理では、seq_indexを使って以下のような操作が行われる可能性があります：

            # 有効なデータの識別: seq_indexを使って、どのデータポイントが有効で、
            # 　　　　　　　　　　どのデータポイントが無効か（例えば、シーケンスの終端にあるパディングなど）を識別します。

            # マスキング: 無効なデータポイントに対しては、アテンションスコアを非常に低く
            # 　　　　　（またはゼロに近く）するためのマスキングを適用します。これにより、
            # 　　　　　　アテンションメカニズムが有効なデータポイントにのみ集中するようになります。

            # アテンション計算: 有効なデータポイントに基づいてアテンションスコアを計算し、
            # 　　　　　　　　　これを使用してコンテキストベクトルを生成します。

            # このようにseq_indexは、DeepMetaNet内で重要な役割を果たしており、
            # モデルが適切なデータポイントに焦点を合わせ、
            # 無関係な情報を無視するための重要な手段となっています。

            # view(-1, self.emb_dim, self.emb_dim)は、
            # PyTorchのテンソル操作であり、
            # テンソルの形状を再構成するために使用されます。
            # この具体的な使用例では、
            # DeepMetaNetからの出力を特定の形状に変形する目的で使われています。

            # view関数の説明
            # view関数は、テンソルの要素数を保持しながら、
            # その形状（次元数と各次元のサイズ）を変更します。
            # 引数には新しい形状を指定します。
            # 
            # view(-1, self.emb_dim, self.emb_dim)
            #
            # -1: この引数は、その次元のサイズを自動的に計算させるために使われます。
            # 　　　PyTorchは他の指定された次元のサイズからこの次元のサイズを推測します。

            # self.emb_dim: この値は、各次元のサイズを表し、ここでは埋め込みの次元数が用いられます。

            # 使用例のコンテキスト
            # この行において、DeepMetaNetのforward関数からの出力がどのような形状であっても、
            # 最終的に[-1, self.emb_dim, self.emb_dim]という形状に変形されることを意味します。

            # この変形は、以下の目的で行われることが一般的です：

            # 適切な形状への整形: モデルの後続の層や操作で必要とされる特定の形状にデータを整形します。
            # 　　　　　　　　　　特に、マトリックス演算を行う際や、特定の形状が要求される層
            #                    （例えば、畳み込み層など）への入力として用いる場合に重要です。

            # データのバッチ処理: -1を使用することで、どのようなバッチサイズのデータであっても、
            # 　　　　　　　　　　指定されたemb_dimに基づいて適切に処理することができます。これにより、
            # 　　　　　　　　　　モデルが異なるサイズの入力データに対して柔軟に対応できるようになります。

            # 次元の明示: この操作によって、データが具体的にどのような情報を持っているか（この場合は、
            # 　　　　　　emb_dim x emb_dimのマトリックスとしての情報）が明確になり、後続の計算や解析が容易になります。

            # このように、view(-1, self.emb_dim, self.emb_dim)はテンソルの形状を効率的に再構成し、
            # モデルの処理フローにおいてデータが適切に扱われるようにする。

            uid_emb = torch.bmm(uid_emb_src, mapping)
            #         torch.bmm が (4) Personalized Bridge
            # uid_emb が Personalized Bridgeの矢印（つまり出力）

            # マッピングの適用:
            # 得られた特徴表現 mapping を uid_emb_src に適用（つまり掛けている）することで、
            # ユーザー埋め込みを変換し、新しいユーザー特徴表現
            # （結局は、ユーザーの１０次元のエンべディング）を生成します。
            # （評価３以上のアイテムの１０次元の情報をユーザーの１０次元のベクトルに転移させている）
            # この操作は、torch.bmm() を使用してバッチ内の行列乗算で行っています。

            # print("uid_emb")
            # print(uid_emb.shape)  #torch.Size([128, 1, 10])

            # uid_emb=
            # tensor([[[-0.2908, -0.0194, -1.0782,  ..., -0.3673, -0.3126, -0.2980]],
            #         [[-0.5410,  0.2967,  0.6124,  ..., -0.1115,  0.2743,  0.3626]],
            #         [[-0.2724,  0.3299, -0.7038,  ..., -0.7070, -0.1771,  0.3543]],
            #         ...,
            #         [[ 0.0030,  0.1357, -0.0112,  ...,  0.0938,  0.3432,  0.0548]],
            #         [[-0.1680,  0.3818,  0.1657,  ...,  0.0492, -0.1270,  0.0651]],
            #         [[ 0.0711, -0.1464, -0.4631,  ..., -0.1480, -0.5205,  0.0798]]],
            #        device='cuda:0', grad_fn=<BmmBackward0>)

            emb = torch.cat([uid_emb, iid_emb], 1)
            # 最終特徴表現の結合と集約:
            # torch.cat([uid_emb, iid_emb], 1) でユーザーとアイテムの埋め込みを結合

            # print("emb")
            # print(emb.shape)  #torch.Size([128, 2, 10])

            # emb=
            # tensor([[[-0.2908, -0.0194, -1.0782,  ..., -0.3673, -0.3126, -0.2980],
            #         [-0.7115, -0.1978, -0.1904,  ..., -0.8180,  1.4446,  1.1038]],

            #         [[-0.5410,  0.2967,  0.6124,  ..., -0.1115,  0.2743,  0.3626],
            #         [ 1.3394,  0.8822,  0.9646,  ..., -2.0942, -0.4185,  0.8425]],
            
            #         [[-0.2724,  0.3299, -0.7038,  ..., -0.7070, -0.1771,  0.3543],
            #         [-0.7705, -0.3640,  0.8693,  ..., -1.4978,  0.2910,  0.7040]],
            #         ...,
            
            #         [[ 0.0030,  0.1357, -0.0112,  ...,  0.0938,  0.3432,  0.0548],
            #         [-1.0656,  0.5583,  0.8929,  ..., -0.4070, -0.9193,  1.2978]],
            
            #         [[-0.1680,  0.3818,  0.1657,  ...,  0.0492, -0.1270,  0.0651],
            #         [ 0.5330,  1.2599, -0.3361,  ...,  0.5620,  0.4383,  0.6068]],
            
            #         [[ 0.0711, -0.1464, -0.4631,  ..., -0.1480, -0.5205,  0.0798],
            #         [ 1.4413, -0.3613, -0.4113,  ...,  0.2031, -0.2558, -0.0691]]],
            
            #     device='cuda:0', grad_fn=<CatBackward0>)


            # torch.sum(emb[:, 0, :] * emb[:, 1, :], dim=1) で最終的な特徴表現を集約しています。
            output = torch.sum(emb[:, 0, :] * emb[:, 1, :], dim=1) 

            # ユーザー埋め込みとアイテム埋め込みの内積で y を求める
            # print("output")
            # print(output.shape)  #torch.Size([128])

            # torch.sum オプションなしなら、行列の中身をすべて加算した結果を返す。
            # dim で和を取る次元を指定できる。
            # 二次元配列の場合、0なら列、1なら行の和を取る。
            # keepdim=True で行列の次元数を減らさないまま結果を返す
            # https://qiita.com/mml/items/c57d503497f420d471cc

            # output=
            # tensor([-1.2861e+00,  4.9089e-01, -1.7177e+00, -4.5503e-01,  5.7384e-02,
            #          2.3089e-01, -1.8103e+00, -5.4718e-01,  2.5760e-01,  6.1811e-03,
            #         -1.8176e-01, -8.5720e-01,  1.9728e-01, -3.9751e-01, -3.6766e-01,
            #          1.6373e-01,  3.4546e-01,  2.0442e+00,  1.2385e+00,  8.0273e-01,
            #         -3.2856e-02,  4.0187e-01,  7.9146e-01,  7.3862e-01,  3.7087e-02,
            #          1.8343e-01,  1.8767e-01,  1.4112e+00,  5.7188e-01, -4.9385e-01,
            #          9.9242e-01,  7.3463e-01, -1.0730e+00,  4.6933e-01,  1.4174e-01,
            #         -6.8725e-01,  5.5742e-02, -8.3083e-01, -8.0948e-01, -6.3083e-01,
            #          1.5317e+00, -6.5590e-01,  1.3942e+00, -1.0775e-01, -8.6151e-01,
            #          5.5352e-01, -2.9939e-01, -4.9169e-01,  7.6654e-01, -2.5314e-01,
            #          2.8441e+00,  2.3875e-01, -1.3147e+00, -1.6066e-01, -1.7200e+00,
            #          8.2952e-01,  9.8863e-02,  7.5430e-01,  3.7472e-01,  6.3267e-02,
            #          1.7214e-01, -9.4913e-01, -2.0699e-03, -1.2153e+00,  2.1401e-01,
            #          6.4217e-01,  4.7725e-01, -4.7759e-01,  4.9463e-01, -9.0781e-01,
            #          5.3925e-01,  4.1198e-01, -3.3587e-01,  1.4943e+00, -1.4276e+00,
            #          3.4840e-02, -2.0203e-01, -6.6639e-01, -1.6759e-01,  8.6432e-01,
            #         -5.9084e-01, -2.1149e+00, -1.6943e+00, -2.1626e-01,  4.1152e+00,
            #          5.4695e-01, -1.0613e-01,  1.7527e-01, -2.7516e-01,  3.4489e-01,
            #         -4.2785e-01,  1.6456e-01,  3.3944e-01, -5.3444e-01,  1.4915e+00,
            #         -1.3877e-01,  5.1104e-01,  3.1920e-01,  1.2904e+00,  4.3196e-01,
            #          4.6682e-01, -7.1608e-01,  4.4942e+00, -7.9036e-01, -1.2330e+00,
            #         -4.1089e-01,  3.0754e-01,  3.4860e-01, -8.1108e-01, -6.7162e-01,
            #          2.8717e+00,  1.0691e+00, -1.8598e-01, -1.4732e-01,  1.1599e+00,
            #         -1.6558e-01, -5.5764e-01,  9.1437e-01,  3.4463e-01, -3.0263e-01,
            #         -2.5881e-01,  7.7463e-03,  3.8159e-01,  4.6935e-01,  3.0095e+00,
            #          6.9053e-01,  4.4426e-01,  2.0208e-01], device='cuda:0',
            #        grad_fn=<SumBackward1>)

            return output

        elif stage == "train_map": #==========EMCDR==========
            # 目的と機能
            # この処理の目的は、ソース域（例えば、あるコンテキストやドメインでのユーザー行動）から
            # 得られたユーザー特徴表現を、ターゲット域（異なるコンテキストやドメイン）の
            # 特徴表現にマッピングすることで学習することにあります。
            # これにより、異なるドメイン間での知識の移転が可能になり、
            # 例えば異なる種類のレコメンデーションシステムや
            # クロスドメインタスクにおいてモデルの効果を向上させることができます。

            # このステージでの学習は、特にマルチタスク学習やドメイン適応、転移学習の文脈で重要です。
            # ユーザーIDから得られるソース特徴表現とターゲット特徴表現がどのように関連しているかを
            # モデルが把握することで、一方のドメインでの学習が他方のドメインでのパフォーマンス向上に
            # 直接貢献する可能性があります。

            # elif stage == 'train_map': の部分では、モデルの Model クラス内の forward 関数が、
            # マッピングのトレーニングステージであることを示しています。
            # このステージでは、特定の埋め込み層を通じてユーザーIDの特徴表現のマッピングを
            # 学習するプロセスが行われています。
            # 具体的には以下の手順で処理が行われます。
            # 処理の詳細

            src_emb = self.src_model.uid_embedding(x.unsqueeze(1)).squeeze()
            # ソースモデルの埋め込み抽出:
            # self.src_model.uid_embedding(x.unsqueeze(1)).squeeze() は、
            # 入力テンソル x （おそらくユーザーIDを示す）をモデルのソース埋め込み層を
            # 通じて変換しています。unsqueeze(1) は次元を追加して正しい形状に調整し、
            # その後 squeeze() で不要な次元を削除しています。

            src_emb = self.mapping.forward(src_emb)
            # マッピング層の適用:
            # 得られたソース埋め込み src_emb に対して、
            # self.mapping 層（線形変換）を適用しています。
            # この線形変換は、ソース埋め込みを新しい特徴空間にマッピングするためのものです。

            tgt_emb = self.tgt_model.uid_embedding(x.unsqueeze(1)).squeeze()
            # ターゲットモデルの埋め込み抽出:
            # 同様に、self.tgt_model.uid_embedding(x.unsqueeze(1)).squeeze() を使用して、
            # 同じユーザーIDのターゲット埋め込みを抽出しています。

            return src_emb, tgt_emb

        elif stage == "test_map":   #==========EMCDR==========
            uid_emb = self.mapping.forward(
                self.src_model.uid_embedding(x[:, 0].unsqueeze(1)).squeeze()
            )
            emb = self.tgt_model.forward(x)
            emb[:, 0, :] = uid_emb
            x = torch.sum(emb[:, 0, :] * emb[:, 1, :], dim=1) # ユーザー埋め込みとアイテム埋め込みを掛けてyを求める
            return x
            # 
            # 
            # 
            # 